---
title: "Workshop Notes"
author: "Dan Tran"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: bootstrap
    df_print: paged
    highlight: zenburn
  pdf_document:
    toc: true
    toc_depth: 3
    df_print: kable
    highlight: zenburn
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MXB107)
```

Workshop Recordings can be found on the Learning Resources Site:
<https://blackboard.qut.edu.au/bbcswebdav/pid-1481644-dt-announcement-rid-58665563_1/courses/MXB107_22se2/_site/videos.html>.

In this document, I'll only write things that are not included in the
official Workshop resources (above).

## Workshop 4 (23/8/22)

### Largest Dice Roll

Imagine everyone in this class is lost in a far-away desert. There is
not enough food, so we decide who get to eat by rolling two dices.

Team A would win if the largest face rolled between the two is 1,2,3 or
4.

Team B win otherwise (if the largest face is 5 or 6).

Let $A$ be the event that team A win, and $B$ the event that team B win.

|       | 1   | 2   | 3   | 4   | 5   | 6   |
|-------|-----|-----|-----|-----|-----|-----|
| **1** | A   | A   | A   | A   | B   | B   |
| **2** | A   | A   | A   | A   | B   | B   |
| **3** | A   | A   | A   | A   | B   | B   |
| **4** | A   | A   | A   | A   | B   | B   |
| **5** | B   | B   | B   | B   | B   | B   |
| **6** | B   | B   | B   | B   | B   | B   |

By counting, we observe that:

$$
\begin{aligned}
Pr(A) &= \dfrac{16}{36} \approx 44.4\% \\
Pr(B) &= \dfrac{20}{36} \approx 55.6\%
\end{aligned}
$$ 

We see that team B actually wins more often than team A.

### Bayesian: Rare Disease

There is a rare yet deadly disease that affects 0.1% of the population.
To diagnose the disease, scientists have made a test kit with 99%
accuracy. This is, if the patient indeed has the disease, it will test
positive 99% of the time (and if you are sick, there is a 1% chance that
it says negative). This also means that even if you are not sick, it can
give positive result 1% of the time.

#### Positive Once

What is the probability that you have the disease given that you tested
positive once?

**Answer**

Let $S$ be the event that you are sick, and $P$ be the event that you
test positive.

$$
\begin{aligned}
Pr(S) &= 0.001 \\
Pr(P | S) &= 0.99 \\
Pr(P | S^c) &= 0.01 \\
Pr(P^c |S) &= 0.01
\end{aligned}
$$ 

We need to find $Pr(S|P)$, Bayesian Theorem would state that:

$$
\begin{aligned}
Pr(S|P) &= \dfrac{Pr(P|S)Pr(S)}{Pr(P)} \\
Pr(P) &= Pr(P|S)Pr(S) + Pr(P|S^c)Pr(S^c) & (\text{Law of total probability}) \\
\\ \text{Substitution:} \\
\Rightarrow Pr(S|P) &= \dfrac{Pr(P|S)Pr(S)}{Pr(P|S)Pr(S) + Pr(P|S^c)Pr(S^c)} \\
&= \dfrac{0.99\times0.001}{0.99\times 0.001 + 0.01\times0.999} \\
&\approx 0.0901\approx9.01\%
\end{aligned}
$$ 

Hence, if you tested positive once, the probability that you actually
have the disease is 9.01% (which is not that bad!).

#### Positive Twice\*

\*This is a challenging question for extension.

Now, you took another test and got a positive result. What is the
probability that you have the disease now?

**Answer**

Renamed the above $P$ to $P_1$, we have:

$$
Pr(S|P_1) \approx 0.0901
$$ For two tests, we have:

$$
Pr(S|P_2 \cap P_1) = \dfrac{Pr(P_2 \cap P_1 | S)Pr(S)}{Pr(P2\cap P_1|S)Pr(S) + Pr(P_2 \cap P_1|S^c)Pr(S^c)}
$$ 

Realise that $P_1$ and $P_2$ are independent, we see that 

$$
Pr(P_2\cap P_1|S)=Pr(P_2|S)Pr(P_1|S)\\
Pr(P_2\cap P_2|S^c)=Pr(P_2|S^c)Pr(P_1|S^c)
$$

Hence,

$$
\begin{aligned}
Pr(S|P_2 \cap P_1) &= \dfrac{Pr(P_2 \cap P_1 | S)Pr(S)}{Pr(P2\cap P_1|S)Pr(S) + Pr(P_2 \cap P_1|S^c)Pr(S^c)}\\
&=\dfrac{Pr(P_2|S)Pr(P_1|S)Pr(S)}{Pr(P_2|S)Pr(P_1|S)Pr(S) + Pr(P_2|S^c)Pr(P_1|S^c)Pr(S^c)}\\
&=\dfrac{0.99\times0.99\times0.001}{0.99\times0.99\times0.001 + 0.01\times0.01\times0.99}\\
&\approx0.9082 \approx 90.82\%
\end{aligned}
$$ 

So the new probability is now 90.82%.

## Workshop 5 (30/8/22)

### Sum of Two Dices

Let X be the random variable generated by summing the results of two
dices.

**a. Find the probability mass function**

|       | 1   | 2   | 3   | 4   | 5   | 6   |
|-------|-----|-----|-----|-----|-----|-----|
| **1** | 2   | 3   | 4   | 5   | 6   | 7   |
| **2** | 3   | 4   | 5   | 6   | 7   | 8   |
| **3** | 4   | 5   | 6   | 7   | 8   | 9   |
| **4** | 5   | 6   | 7   | 8   | 9   | 10  |
| **5** | 6   | 7   | 8   | 9   | 10  | 11  |
| **6** | 7   | 8   | 9   | 10  | 11  | 12  |

: Remember, probability mass function is the probability of each event
X:

$$
p(x)=Pr(X=x), x\in \{2,3,4,5,6,7,8,9,10,11,12\}
$$

By counting, we find that

$$
p(x)=\begin{cases}
\dfrac{1}{36} & ,x=2,12 \\
\dfrac{2}{36} & ,x=3,11 \\
\dfrac{3}{36} & ,x=4,10 \\
\dfrac{4}{36} & ,x=5,9 \\
\dfrac{5}{36} & ,x=6,8 \\
\dfrac{6}{36} & ,x=7 \\
\end{cases}
$$

**b. Find the mean**

$$
\begin{aligned}
E[X] &= \sum_{S}xp(x)\\
&= \sum_{x=2}^{12}xp(x)\\
&= \dfrac{1}{36} \times 2 + \dfrac{2}{36} \times 3 + \dfrac{3}{36} \times 4 + \dfrac{4}{36} \times 5 + \dfrac{5}{36} \times 6 + \dfrac{6}{36} \times 7 \\ &+ \dfrac{5}{36} \times 8 + \dfrac{4}{36} \times 9 + \dfrac{3}{36} \times 10 + \dfrac{2}{36} \times 11 + \dfrac{1}{36} \times 12\\
&= 7
\end{aligned}
$$

Hence the mean is 7.

**c. Find the median**

$$
\text{median of X} = \left\{ x \in S | Pr(X\leq x) \geq \dfrac{1}{2}, Pr(X\geq x) \geq \dfrac{1}{2} \right\}
$$

Try $x = 8$:

$$
\begin{aligned}
Pr(X \leq 8) &= Pr(X=1)+Pr(X=2)+Pr(X=3)+\cdots+Pr(X=8)\\
&= \dfrac{1}{36} + \dfrac{2}{36} + \dfrac{3}{36} + \dfrac{4}{36} + \dfrac{5}{36} + \dfrac{6}{36} + \dfrac{5}{36}\\
\approx&0.72 \geq \dfrac{1}{2}\\
Pr(X \geq 8) &= Pr(X=8)+Pr(X=9)+Pr(X=10)+Pr(X=11)+Pr(X=12)\\
&= \dfrac{5}{36} + \dfrac{4}{36} + \dfrac{3}{36} + \dfrac{2}{36} + \dfrac{1}{36} \\
\approx&0.42 \leq \dfrac{1}{2}
\end{aligned}
$$

Hence $x = 8$ is not the median of X.

Try $x = 7$:

$$
\begin{aligned}
Pr(X \leq 7) &= Pr(X=1)+Pr(X=2)+Pr(X=3)+\cdots+Pr(X=8)\\
&= \dfrac{1}{36} + \dfrac{2}{36} + \dfrac{3}{36} + \dfrac{4}{36} + \dfrac{5}{36} + \dfrac{6}{36}\\
\approx&0.58 \geq \dfrac{1}{2}\\
Pr(X \geq 7) &= Pr(X=8)+Pr(X=9)+Pr(X=10)+Pr(X=11)+Pr(X=12)\\
&= \dfrac{6}{36} + \dfrac{5}{36} + \dfrac{4}{36} + \dfrac{3}{36} + \dfrac{2}{36} + \dfrac{1}{36} \\
\approx&0.58 \geq \dfrac{1}{2}
\end{aligned}
$$

Hence $x = 7$ is the median of X.

**d. Find the mode**

$$
\text{mode of X} = \max_{x \in S}p(x)
$$

In this case, we know

$$
S = \{2,3,4,5,6,7,8,9,10,11,12\}
$$

and based on the $p(x)$ defined above, the most common sum is 7, where
$p(7) = \dfrac{6}{36}$.

Hence the mode is 7.

Since mean = median = mode, we say $X$ is defined by a symmetric,
uni-modal distribution (i.e. no skew).

**e. Find the variance and standard deviation**

$$
\begin{aligned}
Var[X] &= E[(X-\mu]^2]\\
&= \sum_S(x-\mu)^2p(x)\\
&= \sum_{x=2}^{12}(x-7)^2p(x)\\
&= (2-7)^2 \times \dfrac{1}{36} + (3-7)^2 \times \dfrac{2}{36} + (4-7)^2 \times \dfrac{3}{36} + \cdots + (11-7)^2 \times \dfrac{2}{36} + (12-7)^2 \times \dfrac{1}{36}\\
\approx& 5.83\\
\sigma\approx& \sqrt{5.83} \approx2.41
\end{aligned}
$$

Hence the variance of X is 5.83 and the standard deviation is 2.41.

## Workshop 6 (6/9/22)

### Cummulative Probability Distribution

Given a PDF:

$$
f(x) = \dfrac{3x^2}{8}, 0<x<2
$$

Then the CDF is:

$$
\begin{aligned}
F(x) &= \int_{-\infty}^{x}f(u)du\\
&= \int_{-\infty}^{0}f(u)du + \int_{0}^xf(u)du\\
&= 0 + \int_0^xf(u)du\\
&= \int_0^x\dfrac{3u^2}{8}du\\
&= \left[\dfrac{3u^3}{3\times8}\right]_0^x\\
&= \dfrac{x^3}{8} - 0\\
&= \dfrac{x^3}{8}
\end{aligned}
$$

Check if the CDF of the upper limit is 1.

$$
F(2) = \dfrac{2^3}{8} = 1
$$

### Butter weight

Example: A dairy produces packages of butter using a machine that is set
to produce 250 g block of butter with a standard deviation of 10 g. A
sample of size n=13 blocks of butter produce an average weight of
$\bar{x}$=253 g. What is the probability of observing a sample average
weight of 253 g or more?

Set up the solution by hand and use R to compute the probability.

**Answer**

What we have known:

$$
\mu = 250g\\
\sigma = 10g\\
n=13 \text{ blocks}
$$ Apply the central limit theorem:

$$
\bar{x} \sim N\left(\mu, \dfrac{\sigma^2}{n}\right)\\
\bar{x} \sim N\left(250, \dfrac{10^2}{13}\right)\\
$$ In a normal distribution, the first parameter is the mean, and second
is the variance.

And the standard deviation of the distribution of means is $$
\begin{aligned}
s &= \sqrt{\dfrac{\sigma^2}{n}} \\
&= \dfrac{\sigma}{\sqrt{n}} \\
&= \dfrac{10}{\sqrt{13}}
\end{aligned}
$$

To convert from any random variable to Z score:

$$
Z = \dfrac{X - \mu}{s}
$$

Hence:

$$
\begin{aligned}
Pr(\bar{x} \geq 253) &= Pr\left(\dfrac{\bar{x} - \mu}{s} \geq \dfrac{253 - \mu}{s}\right)\\
&= Pr\left(Z \geq \dfrac{253 - \mu}{s}\right)\\
&= Pr\left(Z \geq \dfrac{253 - \mu}{\sigma / \sqrt{n}}\right)\\
&= Pr\left(Z \geq \dfrac{253-250}{10/\sqrt{13}}\right)\\
&= Pr\left(Z \geq 1.08167\right)
\end{aligned}
$$

For a number $t$ up table shows $Pr(Z < t)$. We want
$Pr(Z \geq t) = 1 - Pr(Z < t)$

From lookup table:

$$
Pr(Z < 1.08) \approx 0.85993\\
Pr(Z > 1.08) \approx 1 - 0.85993\approx0.14007
$$

```{r}
pnorm(253, mean = 250, sd = 10/sqrt(13), lower.tail = FALSE)
pnorm(1.08167, mean = 0, sd = 1, lower.tail = FALSE)
```

Therefore, the probability of observing a sample average weight of 253g
or more is approximately 0.14.

### Online Workshop Preference

**Example**

Assume that in Semester 2 of a sample of 100 QUT students living in the
Greater Brisbane Area, 47 replied that they preferred on-line workshops.
Results of previous surveys from Semester 1 indicated that 35% of
students preferred on-line workshops. What is the probability of
observing the proportion from Semester 2, or more given the Semester 1
results are accurate?

The central limit theorem for a sample proportion has that: $$
\hat{p} \sim N\left(p,\dfrac{p(1-p)}{n}\right)
$$

Given:

$$
n = 100\\
p = \dfrac{x}{n} = \dfrac{47}{100}= 0.47\\
$$

Let $\hat{p}$ be the proportion of students who prefer online workshops
in semester 2

Variance of
$\hat{p} = \dfrac{p(1-p)}{n}=\dfrac{0.35\times(1-0.35)}{100}=0.002275$.
And the standard deviation of $\hat{p}$ is $s = \sqrt{0.002275}$

$$
\begin{aligned}
Pr(\hat{p} > 47) &= Pr(\dfrac{\hat{p} - 0.35}{\sqrt{0.002275}} > \dfrac{0.47 - 0.35}{\sqrt{0.002275}})\\
&= Pr(Z > 2.515)\\
&\approx 0.0062 
\end{aligned}
$$

```{r}
pnorm(2.5, lower.tail = FALSE)
```

Therefore, the probability of observing the proportion from semester 2
(0.47) or more, givne the results from semester 1 is accurate is
approximately 0.0062 or 0.62%.

## Workshop 7 (13/9/22)

### Method of Moments

Find the method of moments estimator for the exponential distribution
where

$$
f(x) = \lambda e^{-\lambda x}
$$

First moment is the mean of the sample:

$$
m_1 = \tilde{E[X]}
$$

Where X follows an exponential distribution. We can find E\[X\] in terms
of $\lambda$, then solve for $\lambda$.

$$
E[X] = \int_{_x}{\lambda e^{-\lambda x} dx}\\
= \dfrac{1}{\lambda}
$$

$$
m_1 = \dfrac{1}{\lambda} \\
\lambda = \dfrac{1}{m_1}
$$ Because we need to find an estimation from the sample mean, not an
exact population mean, change the notation accordingly:

$$
\lambda \rightarrow \tilde{\lambda}\\
m_1 \rightarrow \bar{x}
$$

Therefore, the method of moments estimator for the exponential
distribution is:

$$
\tilde{\lambda} = \dfrac{1}{\bar{x}}
$$

### Method of Maximum Likelihood Estimation

$$
f(x) = \lambda e^{-\lambda x}
$$

This gives the maximum likelihood function:

$$
\begin{aligned}
L(\lambda|\mathbf{x}) &= \prod_{i=1}^{n}f(x_i) \\
&=e^{-\lambda x_1}e^{-\lambda x_2}\cdots e^{-\lambda x_n}
\end{aligned}
$$

The log likelihood function:

$$
\begin{aligned}
l(\lambda|\mathbf{x}) &= log(L(\lambda|\mathbf{x})) \\
&= log(\lambda e^{-\lambda x_1}\lambda e^{-\lambda x_2}\cdots \lambda e^{-\lambda x_n})\\
&= log(\lambda e^{-\lambda x_1}) + log(\lambda e^{-\lambda x_2}) + \cdots + log(\lambda e^{-\lambda x_n})\\
&= \sum_{i=1}^{n}log(\lambda e^{-\lambda x_i})\\
&= \sum_{i=1}^{n}(log(\lambda) + log(e^{-\lambda x_i}))\\
&= \sum_{i=1}^{n}(log(\lambda) -\lambda x_i)\\
&= n\log(\lambda) - \lambda \sum_{i=1}^{n}x_i
\end{aligned}
$$

Find the derivative of the log likelihood function:

$$
\begin{aligned}
\dfrac{d}{d\lambda}l(\lambda|\mathbf{x}) &= \dfrac{d}{d\lambda}nlog(\lambda) - \dfrac{d}{d\lambda}(\lambda\sum_{i=1}^{n}x_i) \\
&= n \dfrac{1}{\lambda} - \sum_{i=1}^{n}x_i
\end{aligned}
$$

Set the derivative to 0 and find $\lambda$.

\$\$

```{=tex}
\begin{aligned}

n \dfrac{1}{\lambda} - \sum_{i=1}^{n}x_i &= 0\\
\dfrac{n}{\lambda} &= \sum_{i=1}^{n}x_i\\
\lambda &= \dfrac{n}{\sum_{i=1}^{n}}\\
\lambda &= \dfrac{1}{\dfrac{\sum_{i=1}^{n}x_i}{n}}\\
\lambda &= \dfrac{1}{\bar{x}}
\end{aligned}
```
\$\$

Because we are finding and estimation,
$\lambda \rightarrow \hat{\lambda}$

$$
\hat{\lambda} = \dfrac{1}{\bar{x}}
$$

### Bonus: Finding log likelihood of Poisson

```{=tex}
\begin{aligned}

L(x|\mathbf{x}) &= \prod_{i=1}^{n}\dfrac{\lambda^{x_i}}{x_i!}e^{-\lambda} \\

l(x|\mathbf{x}) &= \sum_{i=1}^{n}log(\dfrac{\lambda^{x_i}}{x_i!}e^{-\lambda})\\
&= \sum_{i=1}^{n}(log(\lambda ^{x_i}) + log(\dfrac{1}{x_i!}) + log(e^{-\lambda}))\\
&= \sum_{i=1}^{n}(x_ilog(\lambda) - log(x_i!) - \lambda)\\
&= log(\lambda)\sum_{i=1}^{n}x_i- \sum_{i=1}^{n}log(x_i!)- n\lambda
\end{aligned}
```

Z-score transformation

$$
Z = \dfrac{X-\mu}{\sigma} \\
Z\sigma=X - \mu
$$

Normal distribution of means from central limit theorem:

$$
\bar{x} \sim N\left(\mu, \dfrac{\sigma^2}{n}\right)\\
Z\dfrac{\sigma}{\sqrt{n}} = \bar{x} - \mu\\
\bar{x} = Z\dfrac{\sigma}{\sqrt n} + \mu
$$

For a type I error rate of $\alpha$, denoted $Z_\alpha$:

$$
Pr(Z > Z_\alpha) = 1 - \alpha
$$

A sample of n=50 adult men reveals that their average daily intake of
protein is x¯=75.6 grams per day with a standard deviation of s=3.5
grams. Construct a 95% confidence interval for the average daily intake
of protein for men.

The confidence interval is

$$
n = 50 \\ \bar{x} = 75.6 \\ s = 3.5\\ \alpha = 0.05 \\

\bar{x} \pm Z\_{\alpha/2}SE\_{\bar{x}} \\

SE\_{\bar{x}} = \dfrac{s}{\sqrt{n}} = \dfrac{3.5}{\sqrt{50}} \\
Z\_{\alpha/2} = Z\_{0.05/2} = 1.96\\

\bar{x} \pm 1.96\\dfrac{3.5}{\\sqrt{50}} \\ \bar{x} \pm 0.970 \\ 75.6
\pm 0.97 \\ (74.63, 76.57)

$$

The 95% confidence interval is (74.63, 76.57).

A random sample of n=985 Queensland residents seeking their opinions on
how the Queensland State Government was handling the COVID-19 crisis. Of
those surveyed, x=592 indicated that they approved of the current
government's handling of the COVID-19 crisis. Construct a 90% confidence
interval for the proportion of the population that approves of the
current government's handling of the COVID-19 crisis.

$$ 
\begin{aligned}
n &= 985\\ 
x &= 592\\ 
\Rightarrow \hat{p} &= \dfrac{x}{n} =
\dfrac{592}{985} \approx 0.60 \\

\alpha &= (1 - 0.9) = 0.1, \text{90% confidence interval} \\

SE\_{\hat{p}} &= \sqrt{\dfrac{\hat{p}(1-\hat{p})}{n}} \\ 
&=\sqrt{\dfrac{0.60(1 - 0.60)}{985}} \\ &\approx 0.0156\\ Z\_{\alpha/2} 
&=Z\_{0.1/2} 
\approx 1.645\\

\hat{p} &\pm Z\_{\alpha/2}SE\_{\hat{p}} \\ 0.60 &\pm 1.645 \times 0.0156
\\ 0.60 &\pm 0.0256\\

&(0.574, 0.625)
\end{aligned}
$$

```{r}
0.6 + 1.645 * 0.0156
```

### Practical Question 2

Find the confidence interval for the mean of the IMDB.Ranking for Star
Trek (The Original Series).

```{r}
library(MXB107)
data(episodes)

original_episodes <- episodes %>% filter(Series.Name == "The Original Series")

# Stating your variables
x_bar <- original_episodes$IMDB.Ranking %>% mean()
n <- nrow(original_episodes)
s <- original_episodes$IMDB.Ranking %>% sd()

# Find Standard Error
se <- s / sqrt(n)
print(se)

# Find critical Z score
alpha <- 0.05
Z_alpha_half <- qnorm(1 - alpha/2)

# Apply confidence interval formula
upper_limit <- x_bar + Z_alpha_half * se
lower_limit <- x_bar - Z_alpha_half * se

# Display results
lower_limit
upper_limit
```

```{r}
params<-episodes%>%
  filter(Series == "TOS")%>%
  summarise(xbar = mean(IMDB.Ranking),std_dev = sd(IMDB.Ranking), N = n())

xbar <- pull(params,xbar)
s <- pull(params,std_dev)
n <- pull(params,N)

SE<-s/sqrt(n)

UCL<-xbar+1.96*SE

LCL<-xbar-1.96*SE

LCL
UCL
```

## Workshop 8 (20/9/22)

### Online Shopping Example

The quality of a product can be reflected by their star ratings.

When browsing for a new pair of Airbuds on JB Hifi, I found two choices
with the following rating distributions.

Sources:
<https://www.jbhifi.com.au/products/lg-tone-free-fp9a-wireless-anc-in-ear-headphones-with-plug-play#reviews>
<https://www.jbhifi.com.au/products/sony-wf-1000xm4-truly-wireless-noise-cancelling-in-ear-headphones-black#reviews>

```{r}
earphones <- data.frame(
  "Ratings" = 1:5,
  "Sony" = c(67, 48, 94, 273, 759),
  "LG" = c(4,5,3,25,109)
)

earphones %>%
  pivot_longer(c("Sony","LG")) %>%
  ggplot(aes(x = Ratings, y=value)) +
  geom_histogram(stat="identity") +
  geom_text(aes(label=value), vjust=-0.25)+
  facet_wrap(~name)+
  xlab("Rating")+
  ylab("Number of customers")+
  ggtitle("Customer Ratings on JB Hifi for LG and Sony Airbuds")
```

Some of their important statistics can also be calculated:

```{r}
alpha <- 0.05
n_lg <- sum(earphones$LG)
n_sony <- sum(earphones$Sony)

averages <- earphones %>% summarise_at(c("Sony","LG"), funs(average = sum(Ratings*.)/sum(.)))
lg_bar <- averages$LG_average
lg_s <- sum((earphones$Ratings - lg_bar)^2 * earphones$LG) / (sum(earphones$LG) - 1)

sony_bar <-averages$Sony_average
sony_s <- sum((earphones$Ratings - sony_bar)^2 * earphones$Sony) / (sum(earphones$Sony) - 1)

Z_critical <- qnorm(1 - alpha)
cat("LG Size:\t", n_lg, "\n")
cat("LG Average:\t", lg_bar, "\n")
cat("LG sd:\t\t", lg_s, "\n")
cat("Sony Size:\t", n_sony, "\n")
cat("Sony Average:\t", sony_bar, "\n")
cat("Sony sd:\t", sony_s, "\n")
```

Which one would you pick?

We all lean towards the Sony one with more ratings, but lower average.

**Go to www.menti.com and use the code 2380 3183**

#### Are they both good?

It is generally believed that a good product should have an average
rating of at least 4.25 (or 85%). Given the reviews above, is there
enough evidence to conclude that both Airbuds are good products, given a
Type I error rate of 0.05?

Since we want to find if a product has greater than or at least 4.25
stars on average. This should be our alternate hypothesis.

$$
H_A = \mu \geq 4.25
$$

That means our null hypothesis is:

$$
H_0 = \mu < 4.25
$$

For a type I error rate of $\alpha = 0.05$, the critical value is:

```{r}
alpha <- 0.05
Z_critical <- qnorm(1 - alpha) # Since this is a one sided test
Z_critical
```

$$
Z_{\alpha} = Z_{0.05} \approx 1.645
$$

Find the test statistics:

a)  For the Sony earbuds:

Sony Size: 1241 Sony Average: 4.296535 Sony sd: 1.241028

$$
\begin{aligned}
n &= 1241\\
\bar{x} &= 4.297\\
s &= 1.241\\
\mu_0 &= 4.25 \\
Z &= \dfrac{\bar{x} - \mu_0}{s/\sqrt{n}}\\
&= \dfrac{4.297 - 4.25}{1.241/\sqrt{1241}}\\
&= 1.334
\end{aligned}
$$

Check if test statistics is greater than critical statistics (because
the null hypothesis has the form $H_0: \mu \leq \mu_0$.

$$
Z > Z_{\alpha}?\\
Z = 1.334 < 1.645
$$

This means our test statistics does not pass the test. We fail to reject
the null hypothesis.

Alternately, we say that the Sony earbuds may not be a good product.

```{r}
(4.297 - 4.25)/(1.241/sqrt(1241))
```

b)  For the LG earbuds:

LG Size: 146 LG Average: 4.575342 LG sd: 0.8253188

$$
\begin{aligned}
n &= 146\\
\bar{x} &= 4.575\\
s&= 0.825\\
\mu_0 &= 4.25\\
Z_\alpha &= 1.645\\
Z &= \dfrac{\bar{x} - \mu_0}{s/\sqrt{n}}\\
&= \dfrac{4.575 - 4.25}{0.825/\sqrt{146}}\\
&\approx 4.760 > Z_{\alpha}
\end{aligned}
$$

We reject the null hypothesis and accept that the LG earbuds is a good
product.

```{r}
(4.575 - 4.25) / (0.825/sqrt(146))
```

#### Which pair is better, statistically?

Perform a test to check if the LG buds are statistically better than the
Sony buds.

Define our alternate hypothesis as:

$$
H_A: \mu_{LG} > \mu_{Sony}\\
H_A: \mu_{LG} - \mu_{Sony} > 0
$$

That means our null hypothesis is:

$$
H_0: \mu_{LG} - \mu_{Sony} \leq 0
$$

This is a one sided test.

LG Size: 146 LG Average: 4.575342 LG sd: 0.8253188 Sony Size: 1241 Sony
Average: 4.296535 Sony sd: 1.241028

$$
\begin{aligned}
\Delta_0 &= 0 \\
\bar{x_{lg}} &=  4.575\\
s_{lg} &= 0.825\\
n_{lg} &= 146\\
\bar{x_{sony}} &= 4.297\\
s_{sony} &= 1.241\\
n_{sony} &= 1241\\
\end{aligned}
$$

Since $\alpha = 0.05$, the critical Z score is the same as above:

$$
Z_\alpha = 1.645
$$

$$
\begin{aligned}
Z &= \dfrac{(\bar{x_{lg}} - \bar{x_{sony}})-\Delta_0}{\sqrt{\dfrac{s_{lg}^2}{n_{lg}}+\dfrac{s_{sony}^2}{n_{sony}}}}\\
&= \dfrac{(4.575 - 4.297) - 0}{\sqrt{\dfrac{0.825^2}{146}+\dfrac{1.241^2}{1241}}}\\
&\approx 3.618
\end{aligned}
$$

```{r}
num <- 4.575 - 4.297
den <- sqrt(0.825^2/146 + 1.241^2/1241)
num/den
```

Test if the test statistics is greater than the critical statistic
($Z_{\alpha} = 1.645$).

$$
Z > Z_\alpha\\
\text{since}\\
3.618 > 1.645
$$

Hence, we reject the null hypothesis and conclude that the LG buds are
better than the Sony buds in terms of ratings.

## Workshop 9 (4/10/22)

### Example 1 Synthetic Diamonds

A new process for producing synthetic diamonds is only profitable if the average weight of the diamonds produced is greater than 0.5 carats. Six samples from the new process are created weighing: (0.46,0.61,0.52,0.48,0.57,0.54) carats. Set up a hypotheses test to determine whether or not the process is profitable. Assume a Type I error rate of α=0.05.

The altnerate hypothesis is "the average weight of the diamonds produced is greater than 0.5 carats".

$$
H_A: \mu > 0.5\\
H_0: \mu \leq 0.5\\
$$

Given variables:

$$
\begin{aligned}
\alpha &= 0.05\\
n &= 6\\
\end{aligned}
$$
Find mean $\bar{x}$ and standard deviation $s$:

$$
\begin{aligned}
\bar{x} &= \dfrac{0.46+0.61+0.52+0.48+0.57+0.54}{6}\\
&= 0.53\\
s &= \sqrt{\dfrac{\sum(x-\bar{x})^2}{n-1}}\\
&= \sqrt{\dfrac{(0.46 - 0.53)^2+(0.61-0.53)^2+(0.52-0.53)^2+(0.48-0.53)^2+(0.57-0.53)^2+(0.54-0.53)^2}{6 - 1}}\\
&\approx 0.056
\end{aligned}
$$

(Or find the mean and standard deviation using R)
```{r}
x <- c(0.46,0.61,0.52,0.48,0.57, 0.54)
<<<<<<< HEAD
=======

>>>>>>> e33dcaf0764a08a6cabd9ed8e9a5fc1fccf40d27
mean(x)
sd(x)
```

The degree of freedom is $\nu=n -1= 6 - 1 = 5$. Therefore, the critical test statistics is $t_{\nu,\alpha} = t_{5, 0.05} = 2.015$ (using the t distribution table).

(Or confirm using r)
```{r}
alpha <- 0.05
nu <- 5
t_critical <- qt(1 - alpha, 5)
t_critical
```

And the test statistics is $T = \sqrt{n}\dfrac{\bar{x}-\mu_0}{s}$.

$$
\begin{aligned}
T &= \sqrt{6}\dfrac{0.53-0.5}{0.056}\\
&= 1.316 < T_{5, 0.05}
\end{aligned}
$$
Since the test statistic is less than the critical test statistics (in the case of $H_0:\mu < \mu_0$, we check if $T > T_{\nu,\alpha}$), we failed to reject the null hypothesis. Hence we conclude that there is not enough evidence to statistically confirm that the diamond-making process is profitable.

### Example 4: Unequal variances

Data are collected to measure the impact strength of two different packaging materials

```{r}
<<<<<<< HEAD
=======

>>>>>>> e33dcaf0764a08a6cabd9ed8e9a5fc1fccf40d27
df <- tibble('Material 1' = c(1.25,1.16,1.33,1.15,1.23,1.20,1.32,1.28,1.21),
              'Material 2' = c(0.89,0.91,0.97,0.95,0.94,0.92,0.98,0.96,0.98))
```

Is there a difference between the strengths of the two packing materials? Test this using the appropriate hypotheses at the Type I Error Rate of α=0.05.

Our alternate hypothesis is that **there is a difference**:

$$
H_A: \mu_1 \neq \mu_2\\
H_A: \mu_1 -\mu_2 \neq \Delta_0
$$

Where in our case, $\Delta_0 = 0$.

$$
\alpha = 0.05\\
n_1 = n_2 = 9
$$

Find the mean and standard deviation using R:

```{r}
stats <- df %>% summarise_at(c("Material 1", "Material 2"), .funs = list(mean = ~mean(.), sd = ~sd(.)))
stats
```

$$
\bar{x_1} \approx 1.24\\
\bar{x_2} \approx 0.94\\
s_1 \approx 0.064\\
s_2 \approx 0.032\\
$$

Check if we need to use unequal variances

$$
\dfrac{max(s^2)}{min(s^2)} > 3 ?\\

\dfrac{s_1^2}{s_2^2} = \dfrac{0.064^2}{0.032^2} \approx 4 > 3
$$
Therefore, we need to use unequal variances.

**Note: If the two variances are equal, use** $\nu = n_1 + n_2 - 2$. Otherwise, use the below formula for unequal variance.

Hence the degree of freedom (for the t test is):

$$
\nu \approx \dfrac{(\dfrac{s_1^2}{n_1} + \dfrac{s_2^2}{n_2})^2}{\dfrac{(s_1^2/n_1)^2}{n_1-1} + \dfrac{(s_2^2/n_2)^2}{n_2-1}}\\
\nu = \dfrac{(\dfrac{0.064^2}{9} + \dfrac{0.032^2}{9})^2}{\dfrac{(0.064^2/9)^2}{9-1} + \dfrac{(0.032^2/9)^2}{9-1}}\\
$$

```{r}
x_1_bar <- stats$`Material 1_mean`
x_2_bar <- stats$`Material 2_mean`
s_1 <- stats$`Material 1_sd`
s_2 <- stats$`Material 2_sd`
n_1 <- 9
n_2 <- 9
<<<<<<< HEAD
num <- (s_1^2/n_1 + s_2^2/n_2)^2
den <- (s_1^2/n_1)^2 / (n_1 - 1) + (s_2^2/n_2)^2 / (n_2 - 1)
=======

num <- (s_1^2/n_1 + s_2^2/n_2)^2
den <- (s_1^2/n_1)^2 / (n_1 - 1) + (s_2^2/n_2)^2 / (n_2 - 1)

>>>>>>> e33dcaf0764a08a6cabd9ed8e9a5fc1fccf40d27
nu <- num / den
nu
```

We found that $\nu \approx 11.73$.

Find the critical test statistics using R:

```{r}
alpha <- 0.05
<<<<<<< HEAD
=======

>>>>>>> e33dcaf0764a08a6cabd9ed8e9a5fc1fccf40d27
t_critical <- qt(1 - alpha / 2, nu) # alpha / 2 because we are using a two-sided test
t_critical
```

So the critical test statistics is $T_{\nu,\alpha/2} \approx 2.18$. We reject the null hypothesis if $T > |T_{\nu,\alpha/2}|$.

Find the test statistics:

$$
\begin{aligned}
T &= \dfrac{(\bar{x_1} - \bar{x_2}) - \Delta_0}{s_p}\\
s_p &= \sqrt{\dfrac{s_1^2}{n_1} + \dfrac{s_2^2}{n_2}}\\
&= \sqrt{\dfrac{0.064^2}{9} + \dfrac{0.032^2}{9}}\\
&\approx 0.024 \\
T &= \frac{1.24 - 0.94}{s_p}\\
&= 12.5 > |2.18|
\end{aligned}
$$

Hence, we reject the null hypothesis and we say that there is a statistically significant difference between the two materials.


```{r}
sqrt(0.064^2/9 + 0.032^2/9)
(1.24 - 0.94) / 0.024
```

### Example 5: Paired Difference

We test the difference between two types of tyre (Type A and Type B) by selecting one tyre of each type at random and then fitting them to the rear wheels of one of five cars. Each car is driven for a specified number of kilometres, and the observed wear is measured.

```{r}
df <- data.frame(A = c(10.6,9.8,12.3,9.7,8.8), B = c(10.2,9.4,11.8,9.1,8.3))
```

$$
\alpha = 0.05\\
n = 5\\
$$

Since each sample (row) also depends on the car, the two groups of measurements are not independent and we can use the paired difference.

```{r}
diff <- data.frame(Difference = df$A - df$B)
diff
```

Form the hypothesises:

$$
H_A: \mu_d \neq 0 \\
H_0: \mu_d = 0
$$


Then we can find the mean and standard deviation of the difference (either by hand or using R):

```{r}
diff %>% summarise_at(c("Difference"), .funs = list(mean = ~mean(.), sd = ~sd(.)))
```

$$
\bar{d} = 0.48\\
s = 0.084\\
$$

Small sample, so need to use t-distribution, where the degree of freedom is $\nu = n - 1=5-1=4$. Hence the critical test statistics (using the t table):

$$
T_{\nu, \alpha/2} = T_{4, 0.025} \approx 2.776
$$

And we would reject the null hypothesis if $T > |T_{\nu, \alpha/2}|$. The test statistics is:

$$
T = \sqrt{n}\dfrac{\bar{d} - 0}{s} \\
= \sqrt{5}\dfrac{0.48}{0.084}\\
= 12.78 > |2.776|
$$
Since $T > |T_{\nu, \alpha/2}|$, we reject the null hypothesis and conclude that there is enough evidence showing that there is a difference between the two tyres.


**Notes**
Confidence interval for the difference in sample means

$$
\bar{x} - \bar{y} \pm T_{\nu,\alpha/2} \sqrt{s_p^2\left(\dfrac{1}{n_x} + \dfrac{1}{n_y}\right)}
$$

## Workshop 10 (11/10/22)

```{r}
y <- c(4.13,4.07,4.04,4.07,4.05,4.04,4.02,4.06,4.10,4.04,
       3.86,3.85,4.08,4.11,4.08,4.01,4.02,4.04,3.97,3.95,
       4.00,4.02,4.01,4.01,4.04,3.99,4.03,3.97,3.98,3.98,
       3.88,3.88,3.91,3.95,3.92,3.97,3.92,3.90,3.97,3.90,
       4.02,3.95,4.02,3.89,3.91,4.01,3.89,3.89,3.99,4.00,
       4.02,3.86,3.96,3.97,4.00,3.82,3.98,3.99,4.02,3.93,
       4.00,4.02,4.03,4.04,4.10,3.81,3.91,3.96,4.05,4.06)
labs <- c(rep("Lab 1", 10),rep("Lab 2", 10), rep("Lab 3",10), rep("Lab 4",10),
          rep("Lab 5",10), rep("Lab 6",10),rep("Lab 7",10))

df <- tibble(y = y, lab = labs)
df %>%
  ggplot(aes(x = lab, y = y)) +
  geom_boxplot()
df
```

```{r}
model <- lm(y ~ lab, data = df)
panderOptions('missing',"")
model%>%aov()%>%pander()
```
```{r}
total_variance <- var(df$y)
total_variance

lab_1_variance <- (df%>%filter(lab=="Lab 1"))$y%>%var()
lab_1_variance
```

```{r}
alpha <- 0.05
F_critical <- qf(1 - alpha, 6, 63)
F_critical
```

According to the ANOVA result table, the test F statistic is 5.66, which is greater than the critical F statistic (begin approximately 2.25). Therefore, we reject the null hypothesis that all the lab means are the same.

We conclude that the means of some labs are different from the others.

```{r}
0.02079 / 0.003673 # Calculating F statistic treatment mean squared error / residuals mean squared error
1 - pf(5.66, 6, 63)
```

### Tukey Honest Significant Difference (HSD) Test

```{r}
anova_model <- model%>%aov()
tukey_model <- anova_model%>%TukeyHSD()

# Round the result to look nicer
tukey_model$lab%>%round(3)
```

According to the Tukey HSD result table, the following pairs are different:

* Lab 4 and Lab 1
* Lab 5 and Lab 1
* Lab 6 and Lab 1
* Lab 4 and Lab 3

### Pr(>F) and hypothesis testing

$$
Pr(F > F_{\alpha}) = \alpha\\
F_{test} > F_{\alpha}?\\
Pr(F > F_{test}) < \alpha
$$

### Mobile Phone Usage

```{r}
df <- tibble(`Usage_Level` = c("Low", "Medium","High"),A = c(27,68,308), B = c(24,76,326), C = c(31,65,312), D = c(23,67,300))
long_df <- df %>% pivot_longer(cols = c("A","B","C","D"))
model_noblocking <- lm(value~name, data = long_df)
anova_model <- model_noblocking%>%aov()
anova_model%>%pander()
long_df
```

Using the non-blocking method, we reject the null hypothesis and conclude that that the prices are not different between different companies.

```{r}
model_blocking <- lm(value~Usage_Level + name, data = long_df)
anova_model_blocking <- model_blocking%>%aov()
anova_model_blocking %>% pander()
```

We see that the Pr(>F) has dropped quite significantly (from 0.9997 to 0.2404), however, since this new value is still much greater than the desired type I error rate of $\alpha = 0.05$, we fail to reject the null hypothesis and conclude that the prices are not really different between different companies.


```{r}
long_df
```

### Two-factor experiment

Similar to blocking, but both variables are observed and intereted in.

```{r}
df <- tibble(Supervisor = c(1,1,1,2,2,2)%>%as.factor(), 
             Day = c(571,610,625,480,516,465), 
             Swing = c(480,474,540,625,600,581),
             Night = c(470,430,450,630,680,661))

long_df <- df %>% pivot_longer(cols = c("Day", "Swing", "Night"), names_to = "ShiftTime")
linear_model <- lm(value ~ Supervisor + ShiftTime + Supervisor:ShiftTime, data = long_df)
linear_model %>% aov() %>%pander()
```

Base on the analysis of variance model for the two factors (Supervisor and ShiftTime), we see that Supervisor has a significant effect on the production (with a Pr(>F) of 0.0002351) and also, the combination of Supervisor and ShiftTime also has a significant effect.

Use Tukey HSD to find the paired difference:

```{r}
aov_model <- linear_model %>% aov()
hsd <- aov_model%>%TukeyHSD()
hsd$"Supervisor:ShiftTime" %>% round(3) %>% pander()
```



Using the significant rows:
* **2:Day-1:Day**     -115   -188.6   -41.41   0.002 : Supervisor 1 is better than 2 during the day (due to the negative difference 2 - 1)
* **1:Night-1:Day**    -152   -225.6   -78.41     0  : super visor 1 does better in day than night shifts (due to the negative difference Night - Day)
* **1:Swing-1:Day**    -104   -177.6   -30.41   0.005: Supervisor 1 does better in day than swing shifts
* **2:Night-2:Day**    170    96.41    243.6      0  : Supervisor 2 does better in night than day shifts
* **2:Swing-2:Day**    115    41.41    188.6    0.002: Supervisor 2 does better in night than swing shifts
* ..

**Key Takeaway: Compare the final columns of result tables (Pr(>F) or p adj) with the type I error rate.**

If it is less than the type I error rate, then conclude there is a significant difference between the means (for ANOVA) or the means of a pair (for TukeyHSD). 

The rest is deciding your linear model (lm):

For one-factor: lm(value ~ Factor)
For blocking: lm(value ~ Factor + Blocking)
For two-factor: lm(value ~ Factor_1 + Factor_2 + Factor_1:Factor_2)

## Workshop 11 (18/10/22)

## Workshop 12 (23/10/22)
